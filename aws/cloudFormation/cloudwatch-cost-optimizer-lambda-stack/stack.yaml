AWSTemplateFormatVersion: '2010-09-09'
Description: Lambda Cloud Stack for Reducing CloudWatch Costs (with Batch Processing)

Parameters:
  LogGroupName:
    Type: String
    Description: CloudWatch Log Group name to optimize cost
  S3BucketName:
    Type: String
    Description: S3 bucket name to store logs
  RetentionInDays:
    Type: Number
    Default: 7
    Description: Retention period (in days) for the CloudWatch Log Group
  BatchSize:
    Type: Number
    Default: 100
    Description: Maximum number of log events processed per Lambda invocation
  BatchWindow:
    Type: Number
    Default: 60
    Description: Maximum batching window in seconds for Lambda

Resources:
  # --- IAM Role for Lambda Function ---
  LogToS3LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LogToS3Policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:*
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub arn:aws:s3:::${S3BucketName}/*

  # --- Lambda Function for Batch Processing and S3 Upload ---
  LogToS3Lambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LogToS3LambdaRole.Arn
      Runtime: python3.12
      Timeout: 120
      Code:
        ZipFile: |
          import boto3
          import gzip
          import base64
          import json
          import os
          import datetime

          s3 = boto3.client('s3')
          def handler(event, context):
              logs = []
              for record in event['records']:
                  payload = base64.b64decode(record['data'])
                  data = gzip.decompress(payload)
                  log_data = json.loads(data)
                  logs.append(log_data)
              # Save batched logs as a single file
              timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H-%M-%S')
              key = f"batch_logs/{timestamp}_{context.aws_request_id}.json"
              s3.put_object(Bucket=os.environ['S3_BUCKET'], Key=key, Body=json.dumps(logs))
              return {'statusCode': 200}

      Environment:
        Variables:
          S3_BUCKET: !Ref S3BucketName

  # --- Lambda Invoke Permission for CloudWatch Logs ---
  LogToS3LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LogToS3Lambda.Arn
      Action: lambda:InvokeFunction
      Principal: logs.amazonaws.com
      SourceArn: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${LogGroupName}:*

  # --- CloudWatch Logs Subscription Filter to Trigger Lambda ---
  LogGroupSubscriptionFilter:
    Type: AWS::Logs::SubscriptionFilter
    DependsOn: LogToS3LambdaInvokePermission
    Properties:
      LogGroupName: !Ref LogGroupName
      FilterPattern: ''
      DestinationArn: !GetAtt LogToS3Lambda.Arn

  # --- Set Retention Policy for Log Group ---
  SetRetentionPolicy:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Ref LogGroupName
      RetentionInDays: !Ref RetentionInDays

Outputs:
  LambdaFunctionName:
    Description: Name of the created Lambda function
    Value: !Ref LogToS3Lambda
  S3Bucket:
    Description: Name of the S3 bucket where logs are stored
    Value: !Ref S3BucketName
